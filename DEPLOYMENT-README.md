# DÃ©ploiement GPT-OSS sur Render

## ğŸš€ DÃ©ploiement Rapide

### Ã‰tape 1 : PrÃ©parer le repo
1. Commitez et pushez vos changements sur GitHub/GitLab
2. Assurez-vous que `render.yaml`, `Dockerfile` et `requirements.txt` sont dans le repo

### Ã‰tape 2 : DÃ©ployer sur Render
1. Allez sur [Render Dashboard](https://dashboard.render.com)
2. Cliquez sur "New +" â†’ "Blueprint"
3. Connectez votre repo Git
4. Render dÃ©tectera automatiquement le `render.yaml`
5. Cliquez "Deploy Blueprint"

### Ã‰tape 3 : Configuration
- **Service Name** : `gpt-oss-api`
- **Plan** : Starter (gratuit pour commencer)
- **Region** : Choisissez Europe (Frankfurt) pour la latence

## ğŸ”§ Configuration AvancÃ©e

### Variables d'Environnement
Ajoutez dans Render Dashboard â†’ Environment :
```
PORT=8000
```

### Backend d'Inference
Par dÃ©faut, utilise `stub` (pas d'infÃ©rence rÃ©elle). Pour changer :
```bash
--inference-backend triton  # Pour Triton
--inference-backend metal   # Pour Metal (macOS)
--inference-backend vllm    # Pour vLLM
```

### ModÃ¨le Checkpoint
Pour charger un vrai modÃ¨le :
```bash
--checkpoint /path/to/model
```
TÃ©lÃ©chargez votre modÃ¨le SafeTensors et montez-le comme volume.

## ğŸŒ Utilisation de l'API

Une fois dÃ©ployÃ©, votre API sera disponible sur :
```
https://gpt-oss-api.onrender.com/v1/responses
```

### Exemple de requÃªte :
```bash
curl -X POST https://gpt-oss-api.onrender.com/v1/responses \
  -H "Content-Type: application/json" \
  -d '{
    "input": "Hello, how are you?",
    "max_output_tokens": 100
  }'
```

## ğŸ“Š Monitoring

- **Logs** : Render Dashboard â†’ Logs
- **MÃ©triques** : Render Dashboard â†’ Metrics
- **Health Check** : `GET /` (retourne 404, mais confirme que l'API tourne)

## ğŸ†˜ DÃ©pannage

### Build qui Ã©choue
- VÃ©rifiez les logs de build dans Render
- Assurez-vous que toutes les dÃ©pendances systÃ¨me sont dans le Dockerfile

### API qui ne rÃ©pond pas
- VÃ©rifiez que le port 8000 est exposÃ©
- VÃ©rifiez les variables d'environnement

### Performance
- Upgrade vers un plan payant pour plus de RAM/CPU
- Utilisez un backend d'inference optimisÃ©

## ğŸ’¡ Alternatives

Si Render ne convient pas :
- **Railway** : `railway up` (encore plus simple)
- **DigitalOcean** : App Platform avec Docker
- **AWS/GCP** : ECS/EKS pour production

---

**ğŸ‰ Votre API GPT-OSS sera dÃ©ployÃ©e et accessible mondialement !**
